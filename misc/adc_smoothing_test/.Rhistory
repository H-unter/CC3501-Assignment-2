training_to_testing_ratio = 0.7
training_indeces = sample(1:nrow(dropout_data), training_to_testing_ratio*nrow(dropout_data))
glm.out <- glm(
Target ~ .,
data = train_data,
subset = training_indeces,
family = binomial
) # summary(glm.out) names(glm.out)
glm.out <- glm(
Target ~ .,
data = droput_data,
subset = training_indeces,
family = binomial
) # summary(glm.out) names(glm.out)
glm.out <- glm(
Target ~ .,
data = dropout_data,
subset = training_indeces,
family = binomial
) # summary(glm.out) names(glm.out)
glm.probs <- predict(glm.out, type = "response") # output probabilities of the form P(Y = Up|X), as opposed to other information such as the logit.
glm.pred <- rep("Dropout", 1421) # create a large vector of drop-out elements
glm.pred[glm.probs > .5] = "Not Dropout" # change to up if P(Up|x)>0.5
glm_classification_table = table(glm.pred, dropout_data$Target)
glm_classification_table = table(glm.pred, dropout_datadropout_data[train_index,]$Target)
glm_classification_table = table(glm.pred, dropout_data[train_index,]$Target)
glm_classification_table = table(glm.pred, dropout_data[training_indeces,]$Target)
glm_classification_table = table(glm.pred, dropout_data[training_indeces,]$Target)
sum(diag(glm_classification_table)) / sum(glm_classification_table)
#-------- generate the model --------
lda.out <- lda( Target ~ .,
data = dropout_data,
subset = training_indeces
) # summary(lda.out) names(lda.out)
# -------- make predictions using model --------
lda.pred <- predict(lda.out, dropout_data)$class
testing_indeces = setdiff(1:nrow(dropout_data), training_indeces)
table(training_indeces+ testing_indeces)
dropout_data_testing = dropout_data[-training_indeces,]
glm.out <- glm(
Target ~ .,
data = dropout_data,
subset = training_indeces,
family = binomial
) # summary(glm.out) names(glm.out)
glm.probs <- predict(glm.out, newdata = dropout_data_testing, type = "response") # output probabilities of the form P(Y = Up|X), as opposed to other information such as the logit.
glm.pred <- rep("Dropout", 1421) # create a large vector of drop-out elements
table(dropout_data_testing$Target)
nrows(dropout_data_testing)
glm.pred <- rep("Dropout", nrow(dropout_data_testing) ) # create a large vector of drop-out elements
glm.pred[glm.probs > .5] = "Not Dropout" # change to up if P(Up|x)>0.5
glm_classification_table = table(glm.pred, dropout_data[training_indeces,]$Target)
sum(diag(glm_classification_table)) / sum(glm_classification_table)
glm_classification_table = table(glm.pred, dropout_data[training_indeces,]$Target)
glm.out <- glm( Target ~ .,
data = dropout_data,
subset = training_indeces,
family = binomial
) # summary(glm.out) names(glm.out)
glm.probs <- predict(glm.out,
newdata = dropout_data_testing,
type = "response") # output probabilities of the form P(Y = Up|X), as opposed to other information such as the logit.
glm.pred <- rep("Dropout", nrow(dropout_data_testing) ) # create a large vector of drop-out elements
glm.pred[glm.probs > .5] = "Not Dropout" # change to up if P(Up|x)>0.5
glm_classification_table = table(glm.pred, dropout_data_testing$Target)
sum(diag(glm_classification_table)) / sum(glm_classification_table)
#-------- generate the model --------
lda.out <- lda( Target ~ .,
data = dropout_data,
subset = training_indeces
) # summary(lda.out) names(lda.out)
# -------- make predictions using model --------
lda.pred <- predict(lda.out, dropout_data_testing)$class
#-------- evaluate model performance --------
lda_classification_table = table(lda.pred, dropout_data$Target)
#-------- generate the model --------
lda.out <- lda( Target ~ .,
data = dropout_data,
subset = training_indeces
) # summary(lda.out) names(lda.out)
# -------- make predictions using model --------
lda.pred <- predict(lda.out, dropout_data_testing)$class
#-------- evaluate model performance --------
lda_classification_table = table(lda.pred, dropout_data$Target)
# -------- make predictions using model --------
lda.pred <- predict(lda.out, dropout_data_testing)$class
# -------- make predictions using model --------
lda.pred <- predict(lda.out, dropout_data_testing)$class
#-------- evaluate model performance --------
lda_classification_table = table(lda.pred, dropout_data_testing$Target)
lda_classification_table
sum(diag(lda_classification_table)) / sum(lda_classification_table)
#-------- generate the model --------
qda.out <- qda( Target ~ .,
data = dropout_data,
subset = training_indeces
) # summary(qda.out) names(qda.out)
#-------- make predictions using model --------
qda.pred <- predict(qda.out, dropout_data_testing)$class
#-------- evaluate model performance --------
qda_classification_table = table(qda.pred, dropout_data_testing$Target)
qda_classification_table
sum(diag(qda_classification_table)) / sum(qda_classification_table)
library(e1071)
#-------- generate the model --------
nb.out <- naiveBayes( Target ~ .,
data = dropout_data
) # summary(nb.out) names(nb.out)
#-------- make predictions using model --------
nb.pred <- predict(nb.out, dropout_data)
#-------- generate the model --------
nb.out <- naiveBayes( Target ~ .,
data = dropout_data,
subset = training_indeces
) # summary(nb.out) names(nb.out)
#-------- make predictions using model --------
nb.pred <- predict(nb.out, dropout_data_training, type = "class")
#-------- make predictions using model --------
nb.pred <- predict(nb.out, dropout_data_training)
#-------- generate the model --------
nb.out <- naiveBayes( Target ~ .,
data = dropout_data,
subset = training_indeces
) # summary(nb.out) names(nb.out)
#-------- make predictions using model --------
nb.pred <- predict(nb.out, dropout_data_testing)
#-------- evaluate model performance --------
nb_classification_table = table(nb.pred, dropout_data_testing$Target)
nb_classification_table
sum(diag(nb_classification_table)) / sum(nb_classification_table)
dropout_data_training = dropout_data[training_indeces,]
#-------- generate the model --------
knn.out = knn( train = as.matrix(dropout_data_training$Target),
test = as.matrix(dropout_data_testing$Target),
cl = dropout_data_training$Target,
k = 3
)
#-------- generate the model --------
knn_training_features = as.matrix(dropout_data_training[ , -ncol(dropout_data_training)])
kns_testing_features = as.matrix(dropout_data_testing[ , -ncol(dropout_data_testing)])
knn.out = knn( train = knn_training_features,
test = kns_testing_features,
cl = dropout_data_training$Target,
k = 3
)
#-------- evaluate model performance --------
knn_classification_table = table(knn.out, dropout_data$Target)
#-------- evaluate model performance --------
knn_classification_table = table(knn.out, dropout_data_testing$Target)
knn_classification_table
sum(diag(knn_classification_table)) / sum(knn_classification_table)
knn_input_k = 1:10
knn.out <- knn( train = knn_training_features,
test = kns_testing_features,
cl = dropout_data_training$Target,
k = k
)
knn_input_k = 1:10
res = sapply(knn_input_k, function(k) {
knn.out <- knn( train = knn_training_features,
test = kns_testing_features,
cl = dropout_data_training$Target,
k = k
)
knn_classification_table = table(knn.out, dropout_data$Target)
sum(diag(knn_classification_table)) / sum(knn_classification_table)
})
res = sapply(knn_input_k, function(k) {
knn.out <- knn( train = knn_training_features,
test = kns_testing_features,
cl = dropout_data_training$Target,
k = k
)
knn_classification_table = table(knn.out, dropout_data_testing$Target)
sum(diag(knn_classification_table)) / sum(knn_classification_table)
})
res = sapply(knn_input_k, function(k) {
knn.out <- knn( train = knn_training_features,
test = kns_testing_features,
cl = dropout_data_training$Target,
k = k
)
knn_classification_table = table(knn.out, dropout_data_testing$Target)
sum(diag(knn_classification_table)) / sum(knn_classification_table)
})
plot(knn_input_k, res, type = "b")
# Define the dissimilarity matrix
D <- matrix(c(0, 0.2, 0.5, 0.6,
0.2, 0, 0.7, 0.3,
0.5, 0.7, 0, 0.8,
0.6, 0.3, 0.8, 0),
nrow = 4, ncol = 4, byrow = TRUE)
# Define the dissimilarity matrix
D <- matrix(c(0, 0.2, 0.5, 0.6,
0.2, 0, 0.7, 0.3,
0.5, 0.7, 0, 0.8,
0.6, 0.3, 0.8, 0),
nrow = 4, ncol = 4, byrow = TRUE)
# Convert the matrix into a distance object
dist_matrix <- as.dist(D)
# Perform hierarchical clustering using complete linkage
hc_complete <- hclust(dist_matrix, method = "complete")
# Plot the dendrogram
plot(hc_complete, main = "Dendrogram using Complete Linkage",
xlab = "Observations", sub = "", ylab = "Height")
# Define the dissimilarity matrix
D <- matrix(c(0, 0.2, 0.5, 0.6,
0.2, 0, 0.7, 0.3,
0.5, 0.7, 0, 0.8,
0.6, 0.3, 0.8, 0),
nrow = 4, ncol = 4, byrow = TRUE)
# Convert the matrix into a distance object
dist_matrix <- as.dist(D)
# Perform hierarchical clustering using complete linkage
hc_complete <- hclust(dist_matrix, method = "average")
# Plot the dendrogram
plot(hc_complete, main = "Dendrogram using Complete Linkage",
xlab = "Observations", sub = "", ylab = "Height")
# Create vectors for the data
x <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
y <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
# Create the linear model
linear_model <- lm(y ~ x)
# Print the summary of the linear model
summary(linear_model)
# Plot the data and the fitted model
plot(x, y, main = "Linear Model of Given Data", xlab = "X", ylab = "Y", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Create vectors for the data
m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
# Create the linear model
linear_model <- lm(y ~ x)
# Print the summary of the linear model
summary(linear_model)
# Plot the data and the fitted model
plot(x, y, main = "Linear Model of Given Data", xlab = "X", ylab = "Y", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Create vectors for the data
m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
# Create the linear model
linear_model <- lm(m ~ v)
# Print the summary of the linear model
summary(linear_model)
# Plot the data and the fitted model
plot(x, y, main = "Linear Model of Given Data", xlab = "X", ylab = "Y", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Create vectors for the data
m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
# Create the linear model
linear_model <- lm(m ~ v)
# Print the summary of the linear model
summary(linear_model)
# Plot the data and the fitted model
plot(x, y, main = "Linear Model of Given Data", xlab = "X", ylab = "Y", pch = 16, col = "blue")
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "m", ylab = "v", pch = 16, col = "blue")
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "m", ylab = "v", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "v", ylab = "m", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Create vectors for the data
m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
# Create the linear model
linear_model <- lm(m ~ v)
# Print the summary of the linear model
summary(linear_model)
# Create vectors for the data
#m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
#v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
m = c(0, 0.2, 0.4, 0.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v = c(0.76, 0.76, 0.76, 0.76, 0.76, 0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
# Create the linear model
linear_model <- lm(m ~ v)
# Print the summary of the linear model
summary(linear_model)
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "v", ylab = "m", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "voltage (V)", ylab = "mass (kg)", pch = 16, col = "blue")
abline(linear_model, col = "red")
# Define R_G as a variable
R_G <- 670  # Example value, you can change this
# Compute the expression
G <- 1 + (49.4 * 10^3 / R_G)
# Compute the expression
G <- 1 + (49.4 * 10^3 / R_G)
m = c(0, 0.2, 0.4, 0.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v_out = c(0.76, 0.76, 0.76, 0.76, 0.76, 0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
v_raw = v_out / G
data = data.frame(
m,
v_raw,
v_out
)
View(data)
resolution = 3.3/(2^12)
# Create vectors for the data
#m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
#v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
deadzone_indeces = 1:5
# Create vectors for the data
#m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
#v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
deadzone_indeces = 1:5
# Define R_G as a variable
R_G <- 670  # Example value, you can change this
# Compute the expression
G <- 1 + (49.4 * 10^3 / R_G)
m = c(0, 0.2, 0.4, 0.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v_out = c(0.76, 0.76, 0.76, 0.76, 0.76, 0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
v_raw = v_out / G
resolution = 3.3/(2^12)
# Create the linear model
linear_model_with_deadzone <- lm(m ~ v)
# Create vectors for the data
#m <- c(2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
#v <- c(0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
deadzone_indeces = 1:5
# Define R_G as a variable
R_G <- 670  # Example value, you can change this
# Compute the expression
G <- 1 + (49.4 * 10^3 / R_G)
m = c(0, 0.2, 0.4, 0.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v_out = c(0.76, 0.76, 0.76, 0.76, 0.76, 0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
v_raw = v_out / G
resolution = 3.3/(2^12)
# Create the linear model
linear_model_with_deadzone <- lm(m ~ v)
linear_model <- lm(m ~ v_raw, subset = -deadzone_indeces)
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "voltage (V)", ylab = "mass (kg)", pch = 16, col = "blue")
abline(linear_model_with_deadzone, col = "green", lty = 2)
abline(linear_model, col = "red", )
legend("topleft", legend = c("With Deadzone", "Without Deadzone"), col = c("green", "red"), lty = c(2, 1), pch = 16)
# Plot the data and the fitted model
plot(v, m, main = "Linear Model of Given Data", xlab = "voltage (V)", ylab = "mass (kg)", pch = 16, col = "blue")
abline(linear_model_with_deadzone, col = "blue", lty = 2)
abline(linear_model, col = "red", )
legend("topleft", legend = c("With Deadzone", "Without Deadzone"), col = c("blue", "red"), lty = c(2, 1), pch = 16)
deadzone_indeces <- which(v_out == v_out[1])
# Define R_G as a variable
R_G <- 670  # Example value, you can change this
# Compute the expression
G <- 1 + (49.4 * 10^3 / R_G)
m = c(0, 0.2, 0.4, 0.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5)
v_out = c(0.76, 0.76, 0.76, 0.76, 0.76, 0.824, 0.89, 0.962, 0.994, 1.02, 1.05, 1.09, 1.14, 1.18, 1.22, 1.24, 1.29, 1.33, 1.37, 1.39, 1.44)
deadzone_indeces <- which(v_out == v_out[1])
v_raw = v_out / G
# Create linear models
linear_model_with_deadzone <- lm(m ~ v_raw)  # With deadzone
linear_model_without_deadzone <- lm(m ~ v_raw, subset = -deadzone_indeces)  # Exclude deadzone
# Plot the data
plot(v_raw, m, main = "Linear Model of Given Data", xlab = "Raw Voltage (V)", ylab = "Mass (kg)", pch = 16, col = "blue")
# Add linear regression lines
abline(linear_model_with_deadzone, col = "blue", lty = 2)  # With deadzone (dashed)
abline(linear_model_without_deadzone, col = "red")  # Without deadzone (solid)
# Add legend
legend("topleft", legend = c("With Deadzone", "Without Deadzone"), col = c("blue", "red"), lty = c(2, 1), pch = 16)
setwd("C:/Users/hkrug/OneDrive - James Cook University/2024 - SP2/CC3501- Computer Interfacing and Control/Assignment 2/misc/adc_smoothing_test")
putty <- read.csv("C:/Users/hkrug/OneDrive - James Cook University/2024 - SP2/CC3501- Computer Interfacing and Control/Assignment 2/misc/adc_smoothing_test/putty.log", header=FALSE)
View(putty)
readings = read.csv(
"putty.log",
header=FALSE
)
View(readings)
readings = read.csv(
"putty.log",
header=TRUE)
readings = read.csv(
"putty.log",
header=TRUE
)
readings = read.csv(
"putty.log",
header=TRUE
)
View(readings)
View(readings)
# Assuming the CSV has columns: voltage, moving_average_.01, moving_average_.05, moving_average_.10, moving_average_.20
readings <- read.csv("putty.log", header=TRUE)
# Plot the raw voltage data
plot(
readings$voltage,
type = "l",       # Line plot
col = "blue",     # Color of raw data
lwd = 2,          # Line width
xlab = "Time",
ylab = "Voltage (V)",
ylim = range(c(readings$voltage, readings$moving_average_.01,
readings$moving_average_.05, readings$moving_average_.10,
readings$moving_average_.20)), # Adjust y-axis to fit all series
main = "Raw and Smoothed Voltages (Exponential Moving Average)"
)
# Add lines for each smoothed moving average level
lines(readings$moving_average_.01, col = "red", lwd = 2)
lines(readings$moving_average_.05, col = "green", lwd = 2)
lines(readings$moving_average_.10, col = "purple", lwd = 2)
lines(readings$moving_average_.20, col = "orange", lwd = 2)
# Assuming the CSV has columns: voltage, moving_average_.01, moving_average_.05, moving_average_.10, moving_average_.20
readings <- read.csv("putty.log", header=TRUE)
pdf("smoothed_readings_plot.pdf", width = 8, height = 6)
# Plot the raw voltage data
plot(
readings$voltage,
type = "l",       # Line plot
col = "blue",     # Color of raw data
lwd = 2,          # Line width
xlab = "Time",
ylab = "Voltage (V)",
ylim = range(c(readings$voltage, readings$moving_average_.01,
readings$moving_average_.05, readings$moving_average_.10,
readings$moving_average_.20)), # Adjust y-axis to fit all series
main = "Raw and Smoothed Voltages (Exponential Moving Average)"
)
# Add lines for each smoothed moving average level
lines(readings$moving_average_.01, col = "red", lwd = 2)
lines(readings$moving_average_.05, col = "green", lwd = 2)
lines(readings$moving_average_.10, col = "purple", lwd = 2)
lines(readings$moving_average_.20, col = "orange", lwd = 2)
# Add a legend to distinguish the series
legend(
"topright",
legend = c("Raw Voltage", "EMA (alpha = 0.01)", "EMA (alpha = 0.05)", "EMA (alpha = 0.10)", "EMA (alpha = 0.20)"),
col = c("blue", "red", "green", "purple", "orange"),
lty = 1,  # Line type
lwd = 2  # Line width
)4
dev.off()
# Add a legend to distinguish the series
legend(
"topright",
legend = c("Raw Voltage", "EMA (alpha = 0.01)", "EMA (alpha = 0.05)", "EMA (alpha = 0.10)", "EMA (alpha = 0.20)"),
col = c("blue", "red", "green", "purple", "orange"),
lty = 1,  # Line type
lwd = 2  # Line width
)4
# Add a legend to distinguish the series
legend(
"topright",
legend = c("Raw Voltage", "EMA (alpha = 0.01)", "EMA (alpha = 0.05)", "EMA (alpha = 0.10)", "EMA (alpha = 0.20)"),
col = c("blue", "red", "green", "purple", "orange"),
lty = 1,  # Line type
lwd = 2  # Line width
)
# Assuming the CSV has columns: voltage, moving_average_.01, moving_average_.05, moving_average_.10, moving_average_.20
readings <- read.csv("putty.log", header=TRUE)
pdf("smoothed_readings_plot.pdf", width = 14, height = 6)
# Plot the raw voltage data
plot(
readings$voltage,
type = "l",       # Line plot
col = "blue",     # Color of raw data
lwd = 2,          # Line width
xlab = "Time",
ylab = "Voltage (V)",
ylim = range(c(readings$voltage, readings$moving_average_.01,
readings$moving_average_.05, readings$moving_average_.10,
readings$moving_average_.20)), # Adjust y-axis to fit all series
main = "Raw and Smoothed Voltages (Exponential Moving Average)"
)
# Add lines for each smoothed moving average level
lines(readings$moving_average_.01, col = "red", lwd = 2)
lines(readings$moving_average_.05, col = "green", lwd = 2)
lines(readings$moving_average_.10, col = "purple", lwd = 2)
lines(readings$moving_average_.20, col = "orange", lwd = 2)
# Add a legend to distinguish the series
legend(
"topright",
legend = c("Raw Voltage", "EMA (alpha = 0.01)", "EMA (alpha = 0.05)", "EMA (alpha = 0.10)", "EMA (alpha = 0.20)"),
col = c("blue", "red", "green", "purple", "orange"),
lty = 1,  # Line type
lwd = 2  # Line width
)
dev.off()
# Assuming the CSV has columns: voltage, moving_average_.01, moving_average_.05, moving_average_.10, moving_average_.20
readings <- read.csv("putty.log", header=TRUE)
pdf("smoothed_readings_plot.pdf", width = 14, height = 6)
# Plot the raw voltage data
plot(
readings$voltage,
type = "l",       # Line plot
col = "blue",     # Color of raw data
lwd = 2,          # Line width
xlab = "Time",
ylab = "Voltage (V)",
ylim = range(c(readings$voltage, readings$moving_average_.01,
readings$moving_average_.05, readings$moving_average_.10,
readings$moving_average_.20)), # Adjust y-axis to fit all series
main = "Raw and Smoothed Voltages (Exponential Moving Average); lower alpha is more smoothing"
)
# Add lines for each smoothed moving average level
lines(readings$moving_average_.01, col = "red", lwd = 2)
lines(readings$moving_average_.05, col = "green", lwd = 2)
lines(readings$moving_average_.10, col = "purple", lwd = 2)
lines(readings$moving_average_.20, col = "orange", lwd = 2)
# Add a legend to distinguish the series
legend(
"topright",
legend = c("Raw Voltage", "EMA (alpha = 0.01)", "EMA (alpha = 0.05)", "EMA (alpha = 0.10)", "EMA (alpha = 0.20)"),
col = c("blue", "red", "green", "purple", "orange"),
lty = 1,  # Line type
lwd = 2  # Line width
)
dev.off()
